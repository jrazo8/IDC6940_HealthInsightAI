---
title: "Diagnosing Diseases using kNN"
subtitle: "An application of kNN to diagnose Diabetes"
author: "Jacqueline Razo (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  revealjs:
    citeproc: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Introduction  {.smaller}

- The k-Nearest-Neighbors (kNN) is an algorithm that is being used in a variety of fields to classify or predict data [@ali2020diabetes]

- It's a simple algorithm that classifies data based on how similar a datapoint is to a class of datapoints. [@zhang2016introduction]

- One of the benefits of using this algorithmic model is how simple it is to use and the fact it’s non-parametric which means it fits a wide variety of datasets.

- One drawback from using this model is that it does have a higher computational cost than other models which means that it doesn’t perform as well or fast on big data [@deng2016efficient]

- In this project we focused on the methodology and application of classification kNN models in the field of healthcare to predict diabetes. 


## Methodology Overview {.smaller}

- The kNN algorithm is a nonparametric supervised learning algorithm that can be used for classification or regression problems. [@syriopoulos2023k]

- In classification, it classifies a datapoint by finding the nearest k data specified and assigning the category based off the majority. 

- Figure 1 illustrates this methodology with two distinct classes of hearts and circles. 

![Figure 1](images/kNN_picture.png){width="200" height="200"}

## Methodology- The classification process {.smaller}

The classification process has three distinct steps:

1. Distance Calculation 
2. Neighbor Selection 
3. Classification decision based on majority voting

## Methodology - Distance calculation {.smaller}

1. Distance calculation
The knn first measures the distance between the datapoint it’s trying to classify and all the training data points. There are different distance calculation methods that can be used but the default and most commonly used method with the kNN is the Euclidean distance formula [@kataria2013review].

$$
d = \sqrt{(X_2 - X_1)^2 + (Y_2 - Y_1)^2}
$$
```{r}
library(ggplot2)

#Add points (X1, Y1) and (X2, Y2)
X1 <- 10; Y1 <- 12
X2 <- 14; Y2 <- 16

#creates a plot
plot(c(X1, X2), c(Y1, Y2), type = "n", xlab = "X-axis", ylab = "Y-axis", main = "Figure 2: Euclidean Distance",xlim = c(X1 - 4, X2 + 4), ylim = c(Y1 - 4, Y2 + 4))

#Plot first point
points(X1, Y1, col = "red", pch = 16, cex = 2) 

#Plot second point
points(X2, Y2, col = "blue", pch = 16, cex = 2)

#Add horizontal line
segments(X1, Y1, X2, Y1, col = "green", lwd = 2)

#Add vertical line 
segments(X2, Y1, X2, Y2, col = "green", lwd = 2)

#Add hypotenuse line
segments(X1, Y1, X2, Y2, col = "purple", lwd = 2, lty = 2)

#Add labels
text(X1, Y1, labels = paste("(X1, Y1)\n(", X1, ",", Y1, ")"), pos = 2, col = "red", cex = 0.7) 
text(X2, Y2, labels = paste("(X2, Y2)\n(", X2, ",", Y2, ")"), pos = 4, col = "blue", cex = 0.7)
text((X1 + X2) / 2 -2, (Y1 + Y2) / 2 + 3, "Euclidean Distance (d)", col = "purple", font = 2, cex = 1.2)
arrows((X1 + X2) / 2, (Y1 + Y2) / 2 + 2,(X1 + X2) / 2, (Y1 + Y2) / 2,col = "purple", lwd = 2, length = 0.1)

#insert formula
text(mean(c(X1, X2)), mean(c(Y1, Y2)) -5, 
     labels = expression(d == sqrt((14 - 10)^2 + (16 - 12)^2)), 
     col = "black", cex = 0.9, font = 1)

text(mean(c(X1, X2)), mean(c(Y1, Y2)) -5, 
     labels = expression(d == sqrt((14 - 10)^2 + (16 - 12)^2)), 
     col = "black", cex = 0.9, font = 1)

```
## Methodology - Neighbor Selection {.smaller}

The kNN allows the selection of a parameter k that is used by the algorithm to choose how many neighbors will be used to classify the unknown datapoint. Studies recommend using cross-validation or heuristic methods, such as setting k to the square root of the dataset size, to determine an optimal value [@zhang2016introduction].

![Figure 1](images/kNN_picture.png){width="200" height="200"}

## Methodology - Majority voting {.smaller}

Once the k-nearest neighbors are identified, the algorithm assigns the new data point the most frequent class label among its neighbors. In cases of ties, distance-weighted voting can be applied, where closer neighbors have higher influence on the classification decision. 

![Figure 1](images/kNN_picture.png){width="200" height="200"}

## Assumptions {.smaller}

- The kNN algorithm assumes similar datapoints will be in close proximity to each other and be neighbors [@zhang2016introduction].

- It also assumes that data points with similar features belong to the same class.
[@boateng2020basic]

## Pre-processing Data {.smaller}
- **Handle missing values**: We must remove the missing values by either inputting them or dropping them to prevent them from skewing the results.
- **Make all values numeric**: All categorical values must be encoded using either one-hot encoding or label encoding.
- **Normalize or Standardize the features**: We must use min-max scaler or the standard scaler to make sure we reduce bias. 
- **Reduce dimensionality**: We can use Principal Component Analysis to reduce the
    number of features but keep the variance.
- **Remove correlated features**: The kNN works best when there aren't
    too many features, so we can use a correlation matrix to see which
    features we can drop. 
- **Fix class imbalance**: Synthetic Minority Over-sampling Technique(SMOTE) can be used to handle class imbalances that can cause biases. 

## Hyperparameter Tuning {.smaller}

In order to increase the accuracy of the model there are a few
parameters that we can adjust.

1.  Find the optimal k parameter: We can use gridsearch to find the best
    parameter k.
2.  Change the distance metric: The kNN uses the euclidean distance by
    default but we can use the Manhattan distance, the Minkowski
    distance or another distance.
3.  Weights: The kNN defaults to a "uniform" weight where it gives the
    same weight to all the distances but it can be adjusted to
    "distance" so that the closest neighbors have more weight.

## Data Exploration and Visualization {.smaller}

-   We explored the [CDC Diabetes Health
Indicators](https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators.)
dataset, sourced from the UC Irvine Machine Learning Repository. It is a
set of data that was gathered by the Centers for Disease Control and
Prevention (CDC) through the Behavioral Risk Factor Surveillance System
(BRFSS), which is one of the biggest continuous health surveys in the
United States.

-   Python and the ucimlrepo package was used to import the dataset directly
from the UCI Machine Learning Repository, following the recommended
instructions. This enabled us to easily save, prepare, and analyze the
data in view of the current research.


## Data Exploration and Visualization - Variables {.smaller}

- The dataset consists of 253,680 survey responses and contains 21 feature variables and 1 binary target variable named **Diabetes_binary**

- **Diabetes_binary**: 0= No Diabetes, 1= Diabetes

- **Binary Variables**: HighBP, HighChol, CholCheck, Smoker, Stroke, HeartDiseaseorAttack, PhysActivity, Fruits, Veggies, HvyAlcoholConsump, AnyHealthcare, NoDocbcCost, DiffWalk, Sex.

- **Ordinal Variables**: GenHlth, MentHlth, PhysHlth, Age, Education, Income

- **Continuous Variables**: BMI


## Data Exploration and Visualization - Mean of Features {.smaller}

Figure 4 shows a graph of the mean of different features in the data. 

```{python}

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ucimlrepo import fetch_ucirepo

# Import dataset
cdc_diabetes_health_indicators = fetch_ucirepo(id=891)

# Combine features and targets into one DataFrame
cdc_data_df = pd.concat([
    cdc_diabetes_health_indicators.data.features,
    cdc_diabetes_health_indicators.data.targets
], axis=1)

# Exclude binary variables
ord_variables = ['GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income', 'BMI']

# Calculate means 
mean_values = cdc_data_df[ord_variables].mean().sort_values(ascending=False)

# Create plot 
plt.figure(figsize=(10, 6))
sns.barplot(x=mean_values.values, y=mean_values.index, palette="plasma")
plt.title("Figure 4: Mean Values of Ordinal and Continuous Variables")
plt.xlabel("Mean")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()


```
## Data Exploration and Visualization - Outliers {.smaller}

Figure 5 shows us outliers in the data that can skew our results 

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ucimlrepo import fetch_ucirepo

# Import dataset
cdc_diabetes_health_indicators = fetch_ucirepo(id=891)

# Combine features and targets into one DataFrame
cdc_data_df = pd.concat([
    cdc_diabetes_health_indicators.data.features,
    cdc_diabetes_health_indicators.data.targets
], axis=1)

# Exclude binary columns
ord_variables = ['GenHlth', 'MentHlth', 'PhysHlth', 'Age', 'Education', 'Income', 'BMI']

# Create boxplot
plt.figure(figsize=(12, 6))
sns.boxplot(data=cdc_data_df[ord_variables], orient="h", palette="Set2")
plt.title("Figure 5: Boxplot showing Outliers for Ordinal and Continuous Variables")
plt.xlabel("Value")
plt.tight_layout()
plt.show()

```
## Data Exploration and Visualization - Class imbalance {.smaller}

Figure 6 shows the class imbalance present in the data

```{python}

import pandas as pd
import matplotlib.pyplot as plt

# Filter binary columns 
binary_cols = [col for col in cdc_data_df.columns 
               if set(cdc_data_df[col].dropna().unique()).issubset({0, 1})]

# Count 0s and 1s
binary_counts = pd.DataFrame({
    '0': (cdc_data_df[binary_cols] == 0).sum(),
    '1': (cdc_data_df[binary_cols] == 1).sum()
})

# Plot
binary_counts.plot(kind='barh', stacked=True, figsize=(10, 8), colormap='Set2')
plt.title("Figure 6: Binary Feature Distribution of 0 and 1")
plt.xlabel("Count")
plt.ylabel("Feature")
plt.legend(title="Class")
plt.tight_layout()
plt.show()

```


## Data Exploration and Visualization - Correlation Analysis {.smaller}

A correlation heatmap was generated in Figure 7 to examine relationships
between variables. The correlation heatmap helps identify strongly
correlated features, which may lead to redundancy in the model.

```{python}

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Compute correlation matrix (MUST BE IN THE SAME CHUNK)
corr_matrix = cdc_data_df.corr()

# Plot the heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5, vmin=-1, vmax=1)
plt.title("Figure 7: Feature Correlation Heatmap")
plt.show()

```

## Data Exploration and Visualization - Key Findings {.smaller}

- There are no missing values, meaning no imputation is needed.

- We have some duplicate values that need to be removed.

- There is a class imbalance with the majority of cases not having diabetes.

## Modeling and Results- Data Preprocessing {.smaller}
- There was no missing data so we didn't have to remove or impute any values.
- We started cleaning the data by dropping these duplicates.
- We kept the ordinal variables the same as they have meaningful natural order that will provide the kNN with meaningful distances.
- We divided the data into testing and training data. We used **test_size=0.2** to use 80% of the data for training the kNN and 20% of the data for testing.
- We chose to standardize them so that BMI and age could be on
the same scale as the other features.

## Modeling and Results - Model Creation {.smaller}

We chose to create three classification kNN models to illustrate the
methodology.

```{r}
#Load gt package
library(gt)

#Create dataframe
model_sum <- data.frame(
  Model = c("Model 1", "Model 2", "Model 3"),
  k = c(5, 15, 15), Weights = c("'uniform'", "'uniform'", "'distance'"),Distance = rep("Euclidean", 3),SMOTE = c("No", "No", "Yes"))

# Create Table 
model_sum %>%
  gt() %>%tab_header(title = md("**Table 4: Model Summary**")
  ) %>%cols_label(Model = "Model Name",k = "k value",Weights = "Weights", Distance = "Distance",SMOTE = "SMOTE") %>%cols_align(align = "center",columns = everything()) %>%tab_options(table.border.top.width = px(2),table.border.bottom.width = px(2),heading.align = "left")
```

```{python, echo=FALSE, include=FALSE}
# Pre-processing for Model 1 and Model 2

# Import libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from ucimlrepo import fetch_ucirepo 

# Load the dataset  
cdc_diabetes_health_indicators = fetch_ucirepo(id=891) 

# Create dataframe by combining features and targets
cdc_data_df = pd.concat(
    [cdc_diabetes_health_indicators.data.features, 
     cdc_diabetes_health_indicators.data.targets],
    axis=1
)

# Drop duplicate rows
cdc_data_df.drop_duplicates(inplace=True)

# Separate features and target 
X = cdc_data_df.drop(columns="Diabetes_binary")
y = cdc_data_df["Diabetes_binary"]

# Split training and testing data with an 80/20 mix
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

```

```{python, echo=FALSE, include=FALSE}
#Extra pre-processing step for Model 3 ONLY
from imblearn.over_sampling import SMOTE

# Apply SMOTE to training data to use for Model 3
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

```

```{python, echo=FALSE, include=FALSE}


from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score

# Model 1: 
knn_model_1 = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='euclidean')
knn_model_1.fit(X_train_scaled, y_train)
y_pred1 = knn_model_1.predict(X_test_scaled)
print(classification_report(y_test, y_pred1))

```

```{python, echo=FALSE, include=FALSE}

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score

# Model 2: 
knn_model_2 = KNeighborsClassifier(n_neighbors=15, weights='uniform', metric='euclidean')
knn_model_2.fit(X_train_scaled, y_train)
y_pred2 = knn_model_2.predict(X_test_scaled)
print(classification_report(y_test, y_pred2))

```

```{python, echo=FALSE, include=FALSE}

# Model 3:
knn_model_3 = KNeighborsClassifier(n_neighbors=15, weights='distance', metric='euclidean')
knn_model_3.fit(X_train_smote, y_train_smote)
y_pred_3 = knn_model_3.predict(X_test_scaled)
print(classification_report(y_test, y_pred_3))

```

## Modeling and Results- Evaluating the models {.smaller}

The table below shows the summary of the three models.

```{python}
# import libraries
import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score

# Create dataframe 
results = pd.DataFrame({
    'Model': ['Model 1', 'Model 2', 'Model 3'],'k': [5, 15, 15],
    'Weight': ['Uniform', 'Uniform', 'Distance'],
    'SMOTE': ['No', 'No', 'Yes'],
    'Accuracy': [
        accuracy_score(y_test, y_pred1),
        accuracy_score(y_test, y_pred2),
        accuracy_score(y_test, y_pred_3)
    ],
    'F1 Score': [
        f1_score(y_test, y_pred1),
        f1_score(y_test, y_pred2),
        f1_score(y_test, y_pred_3)
    ],
    'Precision': [
        precision_score(y_test, y_pred1),
        precision_score(y_test, y_pred2),
        precision_score(y_test, y_pred_3)
    ],
    'Recall': [
        recall_score(y_test, y_pred1),
        recall_score(y_test, y_pred2),
        recall_score(y_test, y_pred_3)
    ],
    'ROC AUC': [
        roc_auc_score(y_test, knn_model_1.predict_proba(X_test_scaled)[:, 1]),
        roc_auc_score(y_test, knn_model_2.predict_proba(X_test_scaled)[:, 1]),
        roc_auc_score(y_test, knn_model_3.predict_proba(X_test_scaled)[:, 1])
    ]
})

# Design the table 
results.style \
    .set_caption("KNN Model Performance Summary") \
    .format({
        "Accuracy": "{:.2%}",
        "F1 Score": "{:.2%}",
        "Precision": "{:.2%}",
        "Recall": "{:.2%}",
        "ROC AUC": "{:.2f}"
    }) \
    .set_properties(**{'text-align': 'center'}) \
    .set_table_styles([
        {'selector': 'caption', 'props': [('caption-side', 'top'), ('font-weight', 'bold'), ('font-size', '16px')]},
        {'selector': 'th', 'props': [('background-color', '#f2f2f2'), ('font-size', '14px')]}
    ])


```
## Modeling and Results {.smaller}

- Model 2 has the highest accuracy at 84.56% but this accuracy score is high because it is good at detecting the non-diabetic cases which are the majority of cases.
- It also has the highest ROC AUC score of 0.77 which means it’s the best model at seperating different classes; however, the recall is 14.56%.

- This means the model is only correctly classifying 14.56% of the actual positive cases for diabetes. 

- Model 3 which has an accuracy of 69.77% and a much higher recall of 69.78%. Model 3 is able to correctly identify about 70% of the positive diabetes cases. 

## Conclusion {.smaller}

- kNN is a promising algorithmic model that can be further improved to detect diabetes. 

- In this project we created three kNN models that were trained to classify unknown datapoints into diabetes or non-diabetes classes using the data from UC Irvines Machine Learning Repository called CDC Diabetes Health indicators. 

- We were able to see how fine tuning a kNN model can help us detect diabetes in the healthcare setting. 

- Model 2 and 3 showed potential with classifying diabetic cases but would need to be furthered improved by being trained with data that shows more dibetic cases if it's going to be used in a healthcare setting.

## References {.smaller}

